{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert raw data to condition-based data\n",
    "\n",
    "* Load the original / raw Ember feature dataset.\n",
    "\n",
    "* Load the trained LGBM boosters.\n",
    "\n",
    "* Extract the list of nodes from the decision tree, contained in the boosters.\n",
    "\n",
    "* Convert dataset with extracted list of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os,sys,pathlib\n",
    "ROOT_PATH = pathlib.Path.cwd().parent.resolve().as_posix()\n",
    "sys.path.insert(0,ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ijcai import extract_nodes_sequences,create_antecedent_datast\n",
    "\n",
    "from inspect import currentframe, getframeinfo\n",
    "from utils import pickle_store, pickle_load,isnotebook\n",
    "from utils import debug_print_tensor\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import toml\n",
    "import timeit\n",
    "import getopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "DATA_DIR    = '/media/data/ijcai_2021/'\n",
    "CONFIG_PATH = ROOT_PATH + '/config/ijcai_2021/'\n",
    "HBRL_CONFIG = 'test_HBRL.toml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(directory,raw_file_name,LGBM_file_name,debug=100,verbose=False):\n",
    "\n",
    "    # load raw data\n",
    "    dic = pickle_load(directory=DATA_DIR,file_name=raw_file_name,verbose=verbose)\n",
    "    N_train = dic['train']['X'][0].shape[0]\n",
    "    N_test  = dic['validation']['X'][0].shape[0]\n",
    "    N       = N_train + N_test\n",
    "    K       = dic['train']['X'][0].shape[1]\n",
    "    print(\"raw data:\")\n",
    "    print(\"    N_train = {}\".format(N_train))\n",
    "    print(\"    N_test  = {}\".format(N_test))\n",
    "    print(\"    N       = {}\".format(N))\n",
    "    print(\"    K       = {}\".format(K))\n",
    "    \n",
    "    # load LGBM data\n",
    "    results = pickle_load(directory=DATA_DIR,file_name=LGBM_file_name,verbose=verbose)\n",
    "    n_splits = len(results['booster'])    # store the data samples\n",
    "\n",
    "    #initialize the dict\n",
    "    data = {'train':{'X':[],'y':[],'X_label':[],'y_label':[]},\n",
    "            'test' :{'X':[],'y':[],'X_label':[],'y_label':[]},\n",
    "            'time(sec)':[]}\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        # set the start time\n",
    "        starttime = timeit.default_timer()\n",
    "\n",
    "        # create a list of nodes\n",
    "        booster = results['booster'][i]\n",
    "        listNodes = extract_nodes_sequences(booster)\n",
    "\n",
    "        # convert training data\n",
    "        X_train = dic['train']['X'][i]\n",
    "        y_train = dic['train']['y'][i]\n",
    "        idx = (y_train != -1)\n",
    "        X_train = X_train[idx,:]\n",
    "        y_train = y_train[idx]\n",
    "        list_X,list_y = create_antecedent_datast(X_train,listNodes,y_train)\n",
    "        data['train']['X_label'].append(list_X[0])\n",
    "        data['train']['y_label'].append(list_y[0])\n",
    "        data['train']['X'].append(list_X[1].astype(int))\n",
    "        data['train']['y'].append(np.transpose(list_y[1].astype(int)))\n",
    "\n",
    "        # convert test data\n",
    "        X_test = dic['validation']['X'][i]\n",
    "        y_test = dic['validation']['y'][i]\n",
    "        idx = (y_test != -1)\n",
    "        X_test = X_test[idx,:]\n",
    "        y_test = y_test[idx]\n",
    "        list_X,list_y = create_antecedent_datast(X_test,listNodes,y_test)\n",
    "        data['test']['X_label'].append(list_X[0])\n",
    "        data['test']['y_label'].append(list_y[0])\n",
    "        data['test']['X'].append(list_X[1].astype(int))\n",
    "        data['test']['y'].append(np.transpose(list_y[1].astype(int)))\n",
    "        data['time(sec)'].append(timeit.default_timer() - starttime)\n",
    "\n",
    "    # store the converted data\n",
    "    N_train = data['train']['X'][0].shape[1]\n",
    "    N_test  = data['test']['X'][0].shape[1]\n",
    "    N       = N_train + N_test\n",
    "    K       = data['train']['X'][0].shape[0]\n",
    "    print(\"converted data:\")\n",
    "    print(\"    N_train = {}\".format(N_train))\n",
    "    print(\"    N_test  = {}\".format(N_test))\n",
    "    print(\"    N       = {}\".format(N))\n",
    "    print(\"    K       = {}\".format(K))\n",
    "    if pickle_store(data,directory=directory,prefix='data_N={}_K={}'.format(N,K),verbose=verbose):\n",
    "        print('successfully stored data.')\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_usage(script_name=None):\n",
    "\n",
    "    \"\"\" \n",
    "    Print the usage of this module\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print('usage: {}.py \\n\\\n",
    "    --config_file <config_file name>\\n\\\n",
    "    --debug <debug level>\\n\\\n",
    "    --verbose <True/False>'.format(script_name))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args=('','')):\n",
    "    '''\n",
    "    Execute the procedures below\n",
    "    '''\n",
    "    # get this script file name\n",
    "    script_name = args[0]\n",
    "    if \"Jupyter\" in script_name:\n",
    "        print('this is from a jupyter nootbook')\n",
    "\n",
    "        # set the file directory and the file name\n",
    "        config_file    = CONFIG_PATH + 'test_HBRL.toml'\n",
    "        config         = toml.load(config_file)\n",
    "        dpath          = config['DataPath']\n",
    "        raw_file_name  = dpath['raw_file_name']\n",
    "        LGBM_file_name = dpath['LGBM_file_name']\n",
    "        \n",
    "        # convert data\n",
    "        convert_dataset(directory=DATA_DIR,raw_file_name=raw_file_name,LGBM_file_name=LGBM_file_name,debug=100,verbose=True)\n",
    "        \n",
    "        return\n",
    "\n",
    "    args = args[1:]\n",
    "    \n",
    "    # read the parameters\n",
    "    params = {'config_file':\"test_HBRL.toml\",'debug':0,'verbose':False}\n",
    "    try:\n",
    "        opts, _args = getopt.getopt(args,\"h\",['config_file=','debug=','verbose='])\n",
    "    except getopt.GetoptError:\n",
    "        print('args={}'.format(args))\n",
    "        print_usage(script_name=script_name)\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print_usage(script_name=script_name)\n",
    "            sys.exit()\n",
    "        elif opt in (\"--config_file\"): params['config_file'] = arg\n",
    "        elif opt in (\"--debug\"):       params['debug']       = int(arg)\n",
    "        elif opt in (\"--verbose\"):     params['verbose']     = True if \"True\" in arg else False\n",
    "        else:\n",
    "            pass\n",
    "    print(\"{}(config_file={},debug={},verbose={}\".format(script_name,params['config_file'],params['debug'],params['verbose']))\n",
    "    \n",
    "    # set the file directory and the file name\n",
    "    config_file    = CONFIG_PATH + params['config_file']\n",
    "    config         = toml.load(config_file)\n",
    "    dpath          = config['DataPath']\n",
    "    raw_file_name  = dpath['raw_file_name']\n",
    "    LGBM_file_name = dpath['LGBM_file_name']\n",
    "    flags          = config['default_flags']\n",
    "    if 'debug' in flag.keys():   debug      = ['debug']\n",
    "    if 'verbose' in flag.keys(): verbose    = ['verbose']\n",
    "\n",
    "    # convert data\n",
    "    convert_dataset(directory=DATA_DIR,raw_file_name=raw_file_name,LGBM_file_name=LGBM_file_name,debug=debug,verbose=verbose)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is from a jupyter nootbook\n",
      "raw data:\n",
      "    N_train = 800\n",
      "    N_test  = 200\n",
      "    N       = 1000\n",
      "    K       = 2381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2381/2381 [00:15<00:00, 154.61it/s]\n",
      "100%|██████████| 2381/2381 [00:15<00:00, 155.35it/s]\n",
      "100%|██████████| 2381/2381 [00:14<00:00, 167.25it/s]\n",
      "100%|██████████| 2381/2381 [00:14<00:00, 167.77it/s]\n",
      "100%|██████████| 2381/2381 [00:16<00:00, 145.26it/s]\n",
      "100%|██████████| 2381/2381 [00:15<00:00, 149.50it/s]\n",
      "100%|██████████| 2381/2381 [00:14<00:00, 161.42it/s]\n",
      "100%|██████████| 2381/2381 [00:14<00:00, 162.96it/s]\n",
      "100%|██████████| 2381/2381 [00:15<00:00, 156.03it/s]\n",
      "100%|██████████| 2381/2381 [00:15<00:00, 157.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted data:\n",
      "    N_train = 800\n",
      "    N_test  = 200\n",
      "    N       = 1000\n",
      "    K       = 518\n",
      "successfully stored data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if isnotebook():\n",
    "        main(args=((\"Jupyter notebook\",\"\")))\n",
    "    else:\n",
    "        main(sys.argv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
