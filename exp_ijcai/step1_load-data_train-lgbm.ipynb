{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data and train LGBM model\n",
    "\n",
    "* Load the original / raw Ember feature dataset.\n",
    "\n",
    "* Split the data for multi-fold cross-validation.\n",
    "\n",
    "* Train LGBM model and predict label probability for validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os,sys\n",
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0,ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random,randint\n",
    "\n",
    "from inspect import currentframe, getframeinfo\n",
    "from utils import pickle_store, pickle_load, debug_print_tensor\n",
    "\n",
    "import ember\n",
    "\n",
    "from ijcai import EMBER_LGBM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import timeit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "N_TRAINING_SAMPLES_TOTAL = 3000\n",
    "N_RAW_FEATURES           = None\n",
    "MODULE_NAME              = \"EMBER\"\n",
    "CONFIG_FILE              = \"prepare_data.toml\"\n",
    "DATA_DIR                 = '/media/data/ijcai_2021/'\n",
    "EMBER_DIR                = '/media/data/ember/ember_2018/'\n",
    "verbose                  = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #1: load the raw data\n",
    "\n",
    "We first load the original data in EMBER feature format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.10.1-bfe5414 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n",
      "X_train (type <class 'numpy.ndarray'>): 600000 samples, 2381 features\n",
      "y_train (type <class 'numpy.ndarray'>): 600000 samples\n",
      "X_test  (type <class 'numpy.ndarray'>): 200000 samples, 2381 features\n",
      "y_test  (type <class 'numpy.ndarray'>): 200000 samples\n",
      "# of features        : 2381\n",
      "# of training samples: 600000, down from 800000\n",
      "# of test samples    : 200000, down from 200000\n"
     ]
    }
   ],
   "source": [
    "feature_version = 2\n",
    "subset = None\n",
    "\n",
    "extractor = ember.PEFeatureExtractor(feature_version)\n",
    "n_features = extractor.dim\n",
    "X_train    = None\n",
    "y_train    = None\n",
    "X_test     = None\n",
    "y_test     = None\n",
    "n_train    = None\n",
    "n_test     = None\n",
    "\n",
    "if subset is None or subset == \"train\":\n",
    "    X_train_path = os.path.join(EMBER_DIR, \"X_train.dat\")\n",
    "    y_train_path = os.path.join(EMBER_DIR, \"y_train.dat\")\n",
    "    y_train = np.memmap(y_train_path, dtype=np.float32, mode=\"r\")\n",
    "    n_train_y = y_train.shape[0]\n",
    "    X_train = np.memmap(X_train_path, dtype=np.float32, mode=\"r\", shape=(n_train_y, n_features))\n",
    "    n_train = X_train.shape[0]\n",
    "    assert(n_train==n_train_y)\n",
    "    idx_train = (y_train!=-1)\n",
    "    X_train = X_train[idx_train,:]\n",
    "    y_train = y_train[idx_train]\n",
    "    (n_train,n_features_train) = X_train.shape\n",
    "    assert(n_features==n_features_train)\n",
    "\n",
    "if subset is None or subset == \"test\":\n",
    "    X_test_path = os.path.join(EMBER_DIR, \"X_test.dat\")\n",
    "    y_test_path = os.path.join(EMBER_DIR, \"y_test.dat\")\n",
    "    y_test = np.memmap(y_test_path, dtype=np.float32, mode=\"r\")\n",
    "    n_test_y = y_test.shape[0]\n",
    "    X_test = np.memmap(X_test_path, dtype=np.float32, mode=\"r\", shape=(n_test_y, n_features))\n",
    "    n_test = X_test.shape[0]\n",
    "    assert(n_test==n_test_y)\n",
    "    idx_test = (y_test!=-1)\n",
    "    X_test = X_test[idx_test,:]\n",
    "    y_test = y_test[idx_test]\n",
    "    (n_test,n_features_test) = X_test.shape\n",
    "    assert(n_features==n_features_test)\n",
    "\n",
    "print(\"X_train (type {:20}): {} samples, {} features\".format(str(type(X_train)),X_train.shape[0],X_train.shape[1]))\n",
    "print(\"y_train (type {:20}): {} samples\".format(str(type(y_train)),y_train.shape[0]))\n",
    "print(\"X_test  (type {:20}): {} samples, {} features\".format(str(type(X_test)),X_test.shape[0],X_test.shape[1]))\n",
    "print(\"y_test  (type {:20}): {} samples\".format(str(type(X_test)),y_test.shape[0]))\n",
    "\n",
    "print(\"# of features        : {}\".format(n_features))\n",
    "print(\"# of training samples: {}, down from {}\".format(n_train,n_train_y))\n",
    "print(\"# of test samples    : {}, down from {}\".format(n_test,n_test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #2: trim the raw data\n",
    "\n",
    "Downsize the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_.shape = (3000, 2381)\n",
      "y_.shape = (3000,)\n"
     ]
    }
   ],
   "source": [
    "# downsize the trainig-validation data samples\n",
    "if N_TRAINING_SAMPLES_TOTAL: n_train_samples   = N_TRAINING_SAMPLES_TOTAL\n",
    "else:                        n_train_samples   = n_train\n",
    "if N_RAW_FEATURES          : n_train_features  = N_RAW_FEATURES\n",
    "else:                        n_train_features  = n_features\n",
    "\n",
    "# pick rules and samples randomly\n",
    "if n_train_samples==n_train:\n",
    "    idx_samples = np.asarray(range(n_train))\n",
    "else:\n",
    "    idx_samples = np.random.choice(np.asarray(range(n_train)),n_train_samples,replace=False)\n",
    "if n_train_samples==n_train:\n",
    "    idx_features = np.asarray(range(n_features))\n",
    "else:\n",
    "    idx_features = np.random.choice(np.asarray(range(n_features)),n_train_features,replace=False)\n",
    "\n",
    "X_ = X_train[idx_samples,:][:,idx_features]\n",
    "y_ = y_train[idx_samples]\n",
    "print(\"X_.shape = {}\".format(X_.shape))\n",
    "print(\"y_.shape = {}\".format(y_.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #3: split the data for cross validation\n",
    "\n",
    "Split the data for multi-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set split conditions\n",
    "n_splits = 5\n",
    "skf   = StratifiedKFold(n_splits=n_splits).split(X_,y_)\n",
    "\n",
    "# split data\n",
    "split_index = {'train':[],'validation':[]}\n",
    "for train_idx, validation_idx in skf:\n",
    "    split_index['train'].append(train_idx)\n",
    "    split_index['validation'].append(validation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted data:\n",
      "    N_train = 2400\n",
      "    N_test  = 600\n",
      "    N       = 3000\n",
      "    K       = 2381\n",
      "save pickle file to /media/data/ijcai_2021//raw_data_N=3000_K=2381_2021-01-18_22:48:46.pkl\n",
      "successfully stored data.\n"
     ]
    }
   ],
   "source": [
    "# store the data samples\n",
    "data = {'train':{'X':[],'y':[]},\n",
    "        'validation' :{'X':[],'y':[]}}\n",
    "\n",
    "for i in range(n_splits):\n",
    "    # select data\n",
    "    train_idx      = split_index['train'][i]\n",
    "    X_train        = X_[train_idx,:]\n",
    "    y_train        = y_[train_idx]\n",
    "    validation_idx = split_index['validation'][i]\n",
    "    X_validation   = X_[validation_idx,:]\n",
    "    y_validation   = y_[validation_idx]\n",
    "    \n",
    "    # set data\n",
    "    data['train']['X'].append(X_train)\n",
    "    data['train']['y'].append(y_train)\n",
    "    data['validation']['X'].append(X_validation)\n",
    "    data['validation']['y'].append(y_validation)\n",
    "\n",
    "# store the converted data\n",
    "N_train = data['train']['X'][i].shape[0]\n",
    "N_test  = data['validation']['X'][i].shape[0]\n",
    "N       = N_train + N_test\n",
    "K       = data['train']['X'][i].shape[1]\n",
    "print(\"converted data:\")\n",
    "print(\"    N_train = {}\".format(N_train))\n",
    "print(\"    N_test  = {}\".format(N_test))\n",
    "print(\"    N       = {}\".format(N))\n",
    "print(\"    K       = {}\".format(K))\n",
    "prefix = 'raw_data_N={}_K={}'.format(N,K)\n",
    "if pickle_store(data,directory=DATA_DIR,prefix=prefix,verbose=verbose):\n",
    "    print('successfully stored data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #4: train LGBM and store booster models\n",
    "\n",
    "Train LGBM models \"boosters\" for all the cross-validation training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir = /media/data/ijcai_2021/\n",
      "Load pickle file from /media/data/ijcai_2021/raw_data_N=3000_K=2381_2021-01-18_22:48:46.pkl\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data        = pickle_load(directory=DATA_DIR,prefix=prefix,verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/funaya/anaconda3/envs/hbrl/lib/python3.8/site-packages/lightgbm/engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ember_datadir': '/media/data/ember/ember_2018/', 'featureversion': 2, 'force_create_vec': False, 'optimize': False, 'modelpath': 'model.txt', 'evals_result': 'None', 'boosting': 'gbdt', 'objective': 'binary', 'learning_rate': 0.05, 'num_threads': 0, 'seed': 'None', 'num_leaves': 30, 'max_depth': -1, 'n_estimators': 10, 'min_data_in_leaf': 20, 'feature_fraction': 0.5}\n",
      "[LightGBM] [Info] Number of positive: 1219, number of negative: 1181\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 169452\n",
      "[LightGBM] [Info] Number of data points in the train set: 2400, number of used features: 1976\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507917 -> initscore=0.031669\n",
      "[LightGBM] [Info] Start training from score 0.031669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ember_datadir': '/media/data/ember/ember_2018/', 'featureversion': 2, 'force_create_vec': False, 'optimize': False, 'modelpath': 'model.txt', 'evals_result': 'None', 'boosting': 'gbdt', 'objective': 'binary', 'learning_rate': 0.05, 'num_threads': 0, 'seed': 'None', 'num_leaves': 30, 'max_depth': -1, 'n_estimators': 10, 'min_data_in_leaf': 20, 'feature_fraction': 0.5}\n",
      "[LightGBM] [Info] Number of positive: 1219, number of negative: 1181\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 169545\n",
      "[LightGBM] [Info] Number of data points in the train set: 2400, number of used features: 1984\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507917 -> initscore=0.031669\n",
      "[LightGBM] [Info] Start training from score 0.031669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:01<00:02,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ember_datadir': '/media/data/ember/ember_2018/', 'featureversion': 2, 'force_create_vec': False, 'optimize': False, 'modelpath': 'model.txt', 'evals_result': 'None', 'boosting': 'gbdt', 'objective': 'binary', 'learning_rate': 0.05, 'num_threads': 0, 'seed': 'None', 'num_leaves': 30, 'max_depth': -1, 'n_estimators': 10, 'min_data_in_leaf': 20, 'feature_fraction': 0.5}\n",
      "[LightGBM] [Info] Number of positive: 1219, number of negative: 1181\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 169876\n",
      "[LightGBM] [Info] Number of data points in the train set: 2400, number of used features: 1986\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507917 -> initscore=0.031669\n",
      "[LightGBM] [Info] Start training from score 0.031669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:02<00:01,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ember_datadir': '/media/data/ember/ember_2018/', 'featureversion': 2, 'force_create_vec': False, 'optimize': False, 'modelpath': 'model.txt', 'evals_result': 'None', 'boosting': 'gbdt', 'objective': 'binary', 'learning_rate': 0.05, 'num_threads': 0, 'seed': 'None', 'num_leaves': 30, 'max_depth': -1, 'n_estimators': 10, 'min_data_in_leaf': 20, 'feature_fraction': 0.5}\n",
      "[LightGBM] [Info] Number of positive: 1219, number of negative: 1181\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 169937\n",
      "[LightGBM] [Info] Number of data points in the train set: 2400, number of used features: 1990\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507917 -> initscore=0.031669\n",
      "[LightGBM] [Info] Start training from score 0.031669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:02<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ember_datadir': '/media/data/ember/ember_2018/', 'featureversion': 2, 'force_create_vec': False, 'optimize': False, 'modelpath': 'model.txt', 'evals_result': 'None', 'boosting': 'gbdt', 'objective': 'binary', 'learning_rate': 0.05, 'num_threads': 0, 'seed': 'None', 'num_leaves': 30, 'max_depth': -1, 'n_estimators': 10, 'min_data_in_leaf': 20, 'feature_fraction': 0.5}\n",
      "[LightGBM] [Info] Number of positive: 1220, number of negative: 1180\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 169922\n",
      "[LightGBM] [Info] Number of data points in the train set: 2400, number of used features: 1996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508333 -> initscore=0.033336\n",
      "[LightGBM] [Info] Start training from score 0.033336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save pickle file to /media/data/ijcai_2021//LGBM_N=3000_E=10_L=30_2021-01-18_22:48:49.pkl\n",
      "experiment results stored successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the dict\n",
    "results = {'booster':[],'y_prob':[],'y_test':[],'time(sec)':[]}\n",
    "\n",
    "# learn the models and derive the label probabilities for the test data\n",
    "for i in tqdm(range(n_splits)):\n",
    "    # load data\n",
    "    X_train = data['train']['X'][i]\n",
    "    y_train = data['train']['y'][i]\n",
    "    X_test  = data['validation']['X'][i]\n",
    "    y_test  = data['validation']['y'][i]\n",
    "\n",
    "    # set the start time\n",
    "    starttime = timeit.default_timer()\n",
    "\n",
    "    # run model training and test\n",
    "    model  = EMBER_LGBM(module_name=MODULE_NAME+'_LGBM', config_file=CONFIG_FILE)\n",
    "    model  = model.fit(X_train,y_train)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # reshape the y_test to (N_test,2), i.e. label vector per category (in this case two categories)\n",
    "    N_test = y_test.shape[0]\n",
    "    y_test = y_test.reshape([N_test,1])\n",
    "    y_test = np.concatenate(((1.0-y_test),y_test),axis=1)\n",
    "\n",
    "    #store the results\n",
    "    results['booster'].append(model.booster)\n",
    "    results['y_prob'].append(y_prob)\n",
    "    results['y_test'].append(y_test)    \n",
    "    results['time(sec)'].append(timeit.default_timer() - starttime)\n",
    "\n",
    "    \n",
    "# set parameters\n",
    "N_total      = X_train.shape[0] + X_test.shape[0]  # the sizes are the same for all cross-validation fold\n",
    "num_leaves   = model.params['num_leaves']['args']\n",
    "n_estimators = model.params['n_estimators']['args']\n",
    "\n",
    "# store the trace file\n",
    "if pickle_store(results,directory=DATA_DIR,module_name='ijcai_2021',prefix='LGBM_N={}_E={}_L={}'.format(N_total,n_estimators,num_leaves),verbose=verbose):\n",
    "    print('experiment results stored successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
